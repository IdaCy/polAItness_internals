[INFO] === Starting inference script with comprehensive logging ===
[INFO] Log file: logs/normal_small_run_progress.log
[INFO] Error log: logs/normal_small_run_errors.log
[INFO] Model name: google/gemma-2-2b
[WARNING] No HF_TOKEN found; proceeding without auth token
[INFO] Loaded 500 samples for inference from prompts/normal.csv.
[INFO] Clearing CUDA cache and setting up GPU memory usage.
[INFO] GPU is available. Setting max_memory={0: '39GB'}
[INFO] Loading tokenizer from google/gemma-2-2b
[INFO] Loading model from google/gemma-2-2b (bfloat16=True, device_map=auto)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.25s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  4.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.61s/it]
[INFO] Model loaded successfully.
[INFO] === Starting inference process ===
[INFO] Processing batch 0 / 500...
`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
[INFO] Inference complete. Activations are stored in 'output/extractions/gemma2b/normal'.
